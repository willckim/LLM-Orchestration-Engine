# ğŸš€ LLM Orchestration Engine

**Enterprise-grade multi-model LLM routing with intelligent cost optimization, observability, and automatic fallbacks.**

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ Overview

A production-ready AI infrastructure service that intelligently routes LLM requests to the optimal model based on task type, cost constraints, latency requirements, and real-time provider health. Built for enterprise scalability with comprehensive observability.

### Why This Project?

Most LLM applications hardcode a single provider. This engine demonstrates enterprise patterns:

- **Multi-model routing** - Automatically select the best model for each task
- **Cost optimization** - Track spending and route to cheaper models when appropriate
- **Resilience** - Automatic fallbacks when providers fail
- **Observability** - Real-time metrics, dashboards, and alerting
- **Scalability** - Serverless-ready architecture (AWS Lambda + Step Functions)

---

## âœ¨ Features

### Intelligent Model Routing
- **Task-aware selection**: Different models excel at different tasks (summarization, sentiment, code, etc.)
- **Preference-based routing**: Optimize for `fast`, `cheap`, `best`, or `balanced`
- **Dynamic scoring**: Real-time model scoring based on cost, latency, quality, and availability
- **Constraint support**: Set max cost or max latency limits per request

### Multi-Provider Support
| Provider | Models | Status |
|----------|--------|--------|
| OpenAI | GPT-4o, GPT-4o-mini, GPT-4-turbo | âœ… Ready |
| Anthropic | Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3 Opus | âœ… Ready |
| Azure OpenAI | GPT-4o, GPT-4o-mini | âœ… Ready |
| AWS Bedrock | Claude, Llama 3 | ğŸ”§ Configured |
| Local ONNX | Sentiment, Classifier | ğŸ”§ Planned |

### Cost Optimization
- Real-time cost tracking per request
- Cost comparison across models
- Savings calculation vs. most expensive alternative
- Budget alerts and limits

### Observability
- Request logging with full context
- P50/P95/P99 latency metrics
- Provider health monitoring
- CloudWatch-compatible metric export
- Real-time dashboard data

### Enterprise Ready
- API key authentication (Cognito-ready)
- Rate limiting support
- Async processing via Step Functions
- Docker + Kubernetes ready
- Terraform infrastructure as code

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Gateway                              â”‚
â”‚                    (FastAPI / Lambda)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Model Router                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Task        â”‚  â”‚ Preference   â”‚  â”‚ Constraint             â”‚  â”‚
â”‚  â”‚ Classifier  â”‚â”€â”€â”‚ Weights      â”‚â”€â”€â”‚ Validator              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Model Scorer                            â”‚  â”‚
â”‚  â”‚  Cost Score â”‚ Latency Score â”‚ Quality Score â”‚ Availability â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  OpenAI  â”‚        â”‚ Anthropicâ”‚        â”‚  Azure   â”‚
    â”‚ Provider â”‚        â”‚ Provider â”‚        â”‚ Provider â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Observability Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Metrics      â”‚  â”‚ Cost          â”‚  â”‚ Request             â”‚   â”‚
â”‚  â”‚ Collector    â”‚  â”‚ Calculator    â”‚  â”‚ Logger              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- (Optional) Docker
- (Optional) API keys for OpenAI/Anthropic

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-orchestration-engine.git
cd llm-orchestration-engine/backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env
# Edit .env with your API keys (optional - works without for mock mode)

# Run the server
uvicorn app.main:app --reload
```

### Docker

```bash
# Build and run
docker-compose up --build

# Or use pre-built image
docker run -p 8000:8000 -e API_KEYS=your-key llm-orchestration-engine
```

---

## ğŸ“– API Usage

### Generate Text

```python
import requests

response = requests.post(
    "http://localhost:8000/api/v1/generate",
    headers={"X-API-Key": "dev-key-123"},
    json={
        "task": "summarize",
        "model_preference": "balanced",
        "text": "Your long text to summarize here..."
    }
)

result = response.json()
print(f"Model used: {result['routing']['selected_model']}")
print(f"Cost: ${result['usage']['total_cost_usd']:.6f}")
print(f"Result: {result['result']}")
```

### Available Tasks

| Task | Description | Best For |
|------|-------------|----------|
| `summarize` | Condense text to key points | Articles, documents |
| `sentiment` | Analyze emotional tone (returns JSON) | Reviews, feedback |
| `rewrite` | Improve clarity and style | Editing, polish |
| `chat` | General conversation | Chatbots, Q&A |
| `code` | Generate/analyze code | Development |
| `analysis` | Deep content analysis | Research, reports |
| `tools` | Tool-calling tasks | Agents, automation |

### Model Preferences

| Preference | Behavior |
|------------|----------|
| `fast` | Prioritize low latency (60% weight on speed) |
| `cheap` | Prioritize low cost (60% weight on price) |
| `best` | Prioritize quality (60% weight on capability) |
| `balanced` | Equal consideration of all factors |

### Response Structure

```json
{
  "success": true,
  "result": "Generated text...",
  "request_id": "req_abc123",
  "routing": {
    "selected_model": "gpt-4o-mini",
    "provider": "openai",
    "reason": "Best cost-latency balance for summarization",
    "cost_score": 0.9,
    "latency_score": 0.85,
    "quality_score": 0.8,
    "final_score": 0.87
  },
  "usage": {
    "input_tokens": 150,
    "output_tokens": 50,
    "total_cost_usd": 0.0000525
  },
  "performance": {
    "total_time_ms": 850,
    "routing_time_ms": 5,
    "inference_time_ms": 820
  }
}
```

---

## ğŸ“Š Observability

### Metrics Dashboard

Access real-time metrics at `GET /api/v1/metrics/summary`:

```json
{
  "requests": {
    "total": 1000,
    "successful": 985,
    "failed": 15
  },
  "latency_ms": {
    "p50": 450,
    "p95": 1200,
    "p99": 2500
  },
  "costs": {
    "total_usd": 1.25,
    "by_model": {"gpt-4o-mini": 0.75, "claude-3-5-haiku": 0.50}
  }
}
```

### Available Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /api/v1/metrics/summary` | Aggregated metrics |
| `GET /api/v1/metrics/models/{model}` | Per-model performance |
| `GET /api/v1/metrics/providers` | Provider health status |
| `GET /api/v1/metrics/costs` | Cost breakdown |
| `GET /api/v1/metrics/logs` | Recent request logs |
| `GET /api/v1/metrics/realtime` | Real-time stats |

---

## ğŸ­ Production Deployment

### AWS Lambda + API Gateway

```bash
# Deploy with Terraform
cd infrastructure/terraform
terraform init
terraform plan -var="environment=prod"
terraform apply
```

### Render (Hybrid Approach)

1. Connect GitHub repository
2. Set environment variables in Render dashboard
3. Deploy with Dockerfile

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `API_KEYS` | Yes | Comma-separated valid API keys |
| `OPENAI_API_KEY` | No | OpenAI API key |
| `ANTHROPIC_API_KEY` | No | Anthropic API key |
| `ENVIRONMENT` | No | `development` or `production` |
| `USE_LOCAL_STORAGE` | No | Use JSON storage vs DynamoDB |

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=app --cov-report=html

# Run specific test file
pytest tests/test_api.py -v
```

---

## ğŸ“ Project Structure

```
llm-orchestration-engine/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration & pricing
â”‚   â”‚   â”œâ”€â”€ models/              # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ routers/             # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ router.py        # Model routing engine
â”‚   â”‚   â”‚   â”œâ”€â”€ providers/       # LLM provider adapters
â”‚   â”‚   â”‚   â”œâ”€â”€ cost_calculator.py
â”‚   â”‚   â”‚   â””â”€â”€ metrics_collector.py
â”‚   â”‚   â””â”€â”€ db/                  # Storage adapters
â”‚   â”œâ”€â”€ lambda_handler.py        # AWS Lambda entry point
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ terraform/               # AWS IaC
â”œâ”€â”€ tests/
â””â”€â”€ docs/
```

---

## ğŸ›£ï¸ Roadmap

- [x] Phase 1: Core routing engine
- [x] Phase 2: Multi-provider support (OpenAI, Anthropic)
- [x] Phase 3: Observability & metrics
- [ ] Phase 4: AWS Lambda deployment
- [ ] Phase 5: Step Functions async processing
- [ ] Phase 6: CloudWatch dashboards
- [ ] Phase 7: Local ONNX models
- [ ] Phase 8: Caching layer

---

## ğŸ¤ Contributing

Contributions welcome! Please read the contributing guidelines first.

---

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

## ğŸ‘¤ Author

Built as an enterprise AI infrastructure portfolio project demonstrating:
- Multi-model LLM orchestration
- Cost optimization strategies
- Production-grade observability
- Serverless AWS architecture
- Clean, maintainable Python

---

*Ready to showcase enterprise AI infrastructure skills? This is it.* ğŸ¯